{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ac6bd91-cf84-40b5-8f21-af9c1416e26e",
   "metadata": {},
   "source": [
    "# Concat of all DEBIAS M SCRIPTS FOR TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903b439e-8903-4ed5-a3bd-b28bd4628716",
   "metadata": {},
   "source": [
    "## Torch Script: base only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7182a5d-cb3d-4d15-861a-d7f5c389e6b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Matplotlib requires dateutil>=2.7; you have 2.2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression, LinearRegression\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LightningModule, Trainer, seed_everything\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/__init__.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m     _logger\u001b[38;5;241m.\u001b[39mpropagate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m seed_everything  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callback  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LightningDataModule, LightningModule  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/callbacks/__init__.py:14\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright The Lightning AI team.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_size_finder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchSizeFinder\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callback\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Checkpoint\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/callbacks/batch_size_finder.py:24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callback\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuner\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_size_scaling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _scale_batch_size\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _TunerExitException, MisconfigurationException\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/callbacks/callback.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m STEP_OUTPUT\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCallback\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Abstract base class used to build new callbacks.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    Subclass this class and override any of the relevant hooks\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/utilities/types.py:40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Metric\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NotRequired, Required\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _TORCH_LRSCHEDULER, LRScheduler, ProcessGroup, ReduceLROnPlateau\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchmetrics/__init__.py:23\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(PIL, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPILLOW_VERSION\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     21\u001b[0m         PIL\u001b[38;5;241m.\u001b[39mPILLOW_VERSION \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39m__version__\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maggregation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     CatMetric,\n\u001b[1;32m     26\u001b[0m     MaxMetric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     SumMetric,\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deprecated\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _PermutationInvariantTraining \u001b[38;5;28;01mas\u001b[39;00m PermutationInvariantTraining  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchmetrics/functional/__init__.py:14\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright The Lightning team.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deprecated\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _permutation_invariant_training \u001b[38;5;28;01mas\u001b[39;00m permutation_invariant_training\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deprecated\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pit_permutate \u001b[38;5;28;01mas\u001b[39;00m pit_permutate\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deprecated\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     _scale_invariant_signal_distortion_ratio \u001b[38;5;28;01mas\u001b[39;00m scale_invariant_signal_distortion_ratio,\n\u001b[1;32m     18\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchmetrics/functional/audio/__init__.py:14\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright The Lightning team.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m permutation_invariant_training, pit_permutate\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     scale_invariant_signal_distortion_ratio,\n\u001b[1;32m     17\u001b[0m     signal_distortion_ratio,\n\u001b[1;32m     18\u001b[0m     source_aggregated_signal_distortion_ratio,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msnr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     complex_scale_invariant_signal_noise_ratio,\n\u001b[1;32m     22\u001b[0m     scale_invariant_signal_noise_ratio,\n\u001b[1;32m     23\u001b[0m     signal_noise_ratio,\n\u001b[1;32m     24\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchmetrics/functional/audio/pit.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rank_zero_warn\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimports\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _SCIPY_AVAILABLE\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# _ps_dict: cache of permutations\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# it's necessary to cache it, otherwise it will consume a large amount of time\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchmetrics/utilities/__init__.py:14\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright The Lightning team.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchecks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_forward_full_state_property\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     dim_zero_cat,\n\u001b[1;32m     17\u001b[0m     dim_zero_max,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     dim_zero_sum,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m class_reduce, reduce\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchmetrics/utilities/checks.py:25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Metric\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m select_topk, to_onehot\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataType\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchmetrics/metric.py:41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gather_all_tensors\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TorchMetricsUserError\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _AX_TYPE, _PLOT_OUT_TYPE, plot_single_or_multi_val\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprints\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rank_zero_warn\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjit_distributed_available\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchmetrics/utilities/plot.py:25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimports\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _LATEX_AVAILABLE, _MATPLOTLIB_AVAILABLE, _SCIENCEPLOT_AVAILABLE\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _MATPLOTLIB_AVAILABLE:\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maxes\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/__init__.py:227\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m parse_version(module\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m parse_version(minver):\n\u001b[1;32m    223\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatplotlib requires \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mminver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    224\u001b[0m                               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 227\u001b[0m _check_versions()\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# The decorator ensures this always returns the same handler (and it is only\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# attached once).\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache()\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ensure_handler\u001b[39m():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/__init__.py:223\u001b[0m, in \u001b[0;36m_check_versions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(modname)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parse_version(module\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m parse_version(minver):\n\u001b[0;32m--> 223\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatplotlib requires \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mminver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    224\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: Matplotlib requires dateutil>=2.7; you have 2.2"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.functional import softmax, pairwise_distance\n",
    "from torch.optim import Adam\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "from pl_bolts.datamodules import SklearnDataModule\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from typing import Any, Dict, List, Tuple, Type\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e597793-7dc7-4d53-a876-d37c3c6df8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(xx):\n",
    "    return(xx/xx.sum(axis=1)[:, np.newaxis] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baac4d7c-67c5-4357-8343-0d1716c1938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PL_DEBIASM(pl.LightningModule):\n",
    "    \"\"\"Logistic regression model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        X, \n",
    "        batch_sim_strength: float,\n",
    "        input_dim: int,\n",
    "        num_classes: int,\n",
    "        bias: bool = True,\n",
    "        learning_rate: float = 1e-4,\n",
    "        optimizer: Type[Optimizer] = Adam,\n",
    "        l1_strength: float = 0.0,\n",
    "        l2_strength: float = 0.0,\n",
    "        w_l2 : float = 0.0,\n",
    "        use_log: bool=False,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: number of dimensions of the input (at least 1)\n",
    "            num_classes: number of class labels (binary: 2, multi-class: >2)\n",
    "            bias: specifies if a constant or intercept should be fitted (equivalent to fit_intercept in sklearn)\n",
    "            learning_rate: learning_rate for the optimizer\n",
    "            optimizer: the optimizer to use (default: ``Adam``)\n",
    "            l1_strength: L1 regularization strength (default: ``0.0``)\n",
    "            l2_strength: L2 regularization strength (default: ``0.0``)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() # save hyperparams upon init\n",
    "        self.optimizer = optimizer # Adam optimizer\n",
    "        \n",
    "        # main model component: check hparams for input\n",
    "        self.linear = nn.Linear(in_features=self.hparams.input_dim, \n",
    "                                out_features=self.hparams.num_classes, \n",
    "                                bias=bias) \n",
    "\n",
    "        self.X=X[:, 1:] # all cols except batch number\n",
    "        self.bs=X[:, 0].long() # batch number in long datatype\n",
    "        self.unique_bs=self.bs.unique().long() # set of batch values (starts at 0)\n",
    "        self.n_batches=self.unique_bs.max()+1 # number of unique batches\n",
    "        self.batch_weights = torch.nn.Parameter(data = torch.zeros(self.n_batches,\n",
    "                                                                   input_dim)) # init batch weights as 0's (not 1's?)\n",
    "\n",
    "        self.batch_sim_str=batch_sim_strength # this is the batch similarity metric\n",
    "        \n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        batch_inds, x = x[:, 0], x[:, 1:]\n",
    "\n",
    "        # x gets normalized by batch weights here! Can I just return this instead?\n",
    "        # ^^^ No! This seems to skip learning entirely.\n",
    "        x = F.normalize( torch.pow(2, self.batch_weights[batch_inds.long()] ) * x, p=1 )\n",
    "        x = self.linear(x)\n",
    "        y_hat = softmax(x)\n",
    "        return y_hat\n",
    "\n",
    "    \n",
    "    def training_step(self, batch: Tuple[Tensor, Tensor], batch_idx: int) -> Dict[str, Tensor]:\n",
    "        x, y = batch\n",
    "        \n",
    "        # flatten any input\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        y_hat = self.forward(x)\n",
    "        \n",
    "        \n",
    "        # PyTorch cross_entropy function combines log_softmax and nll_loss in single function\n",
    "        loss = F.cross_entropy(y_hat, y, reduction=\"sum\")\n",
    "        \n",
    "        # L1 regularizer\n",
    "        if self.hparams.l1_strength > 0:\n",
    "            l1_reg = self.linear.weight.abs().sum()\n",
    "            loss += self.hparams.l1_strength * l1_reg\n",
    "\n",
    "        # L2 regularizer\n",
    "        if self.hparams.l2_strength > 0:\n",
    "            l2_reg = self.linear.weight.pow(2).sum()\n",
    "            loss += self.hparams.l2_strength * l2_reg\n",
    "            \n",
    "        # L2 regularizer\n",
    "        if self.hparams.l2_strength > 0:\n",
    "            l2_reg = self.linear.weight.pow(2).sum()\n",
    "            loss += self.hparams.l2_strength * l2_reg\n",
    "            \n",
    "        # DOMAIN similarity regularizer / bias correction\n",
    "        if self.batch_sim_str > 0:\n",
    "            x1 = torch.stack( [ ( torch.pow(2, self.batch_weights\\\n",
    "                                          )[torch.where(self.unique_bs==a)[0]] * \\\n",
    "                    (self.X[ torch.where(self.bs==a)[0] ] )  \\\n",
    "                   ).mean(axis=0) for a in self.unique_bs ] )\n",
    "\n",
    "            x1 = F.normalize(x1, p=1)\n",
    "\n",
    "            loss += sum( [pairwise_distance(x1, a) for a in x1] ).sum() *\\\n",
    "                                    self.batch_sim_str\n",
    "            \n",
    "        # L2 regularizer for bias weight    \n",
    "        if self.hparams.w_l2 > 0:\n",
    "            # L2 regularizer for weighting parameter\n",
    "            l2_reg = self.batch_weights.pow(2).sum()\n",
    "            loss += self.hparams.w_l2 * l2_reg\n",
    "\n",
    "\n",
    "\n",
    "        loss /= float( x.size(0) )\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch: Tuple[Tensor, Tensor], batch_idx: int) -> Dict[str, Tensor]:\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        y_hat = self.forward(x)\n",
    "         # PyTorch cross_entropy function combines log_softmax and nll_loss in single function\n",
    "        loss = F.cross_entropy(y_hat, y, reduction=\"sum\")\n",
    "        \n",
    "        # L1 regularizer\n",
    "        if self.hparams.l1_strength > 0:\n",
    "            l1_reg = self.linear.weight.abs().sum()\n",
    "            loss += self.hparams.l1_strength * l1_reg\n",
    "\n",
    "        # L2 regularizer\n",
    "        if self.hparams.l2_strength > 0:\n",
    "            l2_reg = self.linear.weight.pow(2).sum()\n",
    "            loss += self.hparams.l2_strength * l2_reg\n",
    "        \n",
    "        self.log('val_loss', loss)\n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs: List[Dict[str, Tensor]]) -> Dict[str, Tensor]:\n",
    "        acc = 0\n",
    "        val_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        \n",
    "        return {\"val_loss\": val_loss}\n",
    "\n",
    "    def test_step(self, batch: Tuple[Tensor, Tensor], batch_idx: int) -> Dict[str, Tensor]:\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        y_hat = self.forward(x)\n",
    "        acc = 0#\n",
    "        return {\"test_loss\": F.cross_entropy(y_hat, y),\n",
    "                \"acc\": acc}\n",
    "\n",
    "    def test_epoch_end(self, outputs: List[Dict[str, Tensor]]) -> Dict[str, Tensor]:\n",
    "        acc = 0\n",
    "        test_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"test_ce_loss\": test_loss, \"test_acc\": acc}\n",
    "        progress_bar_metrics = tensorboard_logs\n",
    "        return {\"test_loss\": test_loss,\n",
    "                \"log\": tensorboard_logs,\n",
    "                \"progress_bar\": progress_bar_metrics}\n",
    "\n",
    "    def configure_optimizers(self) -> Optimizer:\n",
    "        return self.optimizer(self.parameters(), lr=self.hparams.learning_rate)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n",
    "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument(\"--learning_rate\", type=float, default=0.0001)\n",
    "        parser.add_argument(\"--input_dim\", type=int, default=None)\n",
    "        parser.add_argument(\"--num_classes\", type=int, default=None)\n",
    "        parser.add_argument(\"--bias\", default=\"store_true\")\n",
    "        parser.add_argument(\"--batch_size\", type=int, default=100)\n",
    "        \n",
    "        return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24468dfe-206f-46a0-a945-4b1e9bddbc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DEBIASM_train_and_pred(X_train, \n",
    "                         X_val, \n",
    "                         y_train, \n",
    "                         y_val,\n",
    "                         batch_sim_strength=1,\n",
    "                         w_l2 = 0,\n",
    "                         batch_size=None,\n",
    "                         learning_rate=0.005,\n",
    "                         l2_strength=0,\n",
    "                         includes_batches=False,\n",
    "                         val_split=0.1, \n",
    "                         test_split=0,\n",
    "                         min_epochs=15, \n",
    "#                          use_log=False, \n",
    "                         verbose=False\n",
    "                         ):\n",
    "    \n",
    "    if batch_size is None:\n",
    "        batch_size=X_train.shape[0]\n",
    "    \n",
    "    baseline_mod = LogisticRegression(max_iter=2500)\n",
    "    baseline_mod.fit(rescale( X_train[:, 1:]), y_train)\n",
    "        \n",
    "        \n",
    "    model = PL_DEBIASM( X = torch.tensor( np.vstack((X_train, X_val)) ),\n",
    "                      batch_sim_strength = batch_sim_strength,\n",
    "                      input_dim = X_train.shape[1]-1, \n",
    "                      num_classes = 2, \n",
    "                      batch_size = batch_size,\n",
    "                      learning_rate = learning_rate,\n",
    "                      l2_strength = l2_strength, \n",
    "                      w_l2 = w_l2\n",
    "                    )\n",
    "    \n",
    "    ## initialize parameters to lbe similar to standard logistic regression\n",
    "    model.linear.weight.data[0]=-torch.tensor( baseline_mod.coef_[0] )\n",
    "    model.linear.weight.data[1]= torch.tensor( baseline_mod.coef_[0] )\n",
    "\n",
    "    ## build pl dataloader\n",
    "    dm = SklearnDataModule(X_train, \n",
    "                           y_train.astype(int),\n",
    "                           val_split=val_split,\n",
    "                           test_split=test_split\n",
    "                           )\n",
    "\n",
    "    ## run training\n",
    "    trainer = pl.Trainer(callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=2)], \n",
    "                             check_val_every_n_epoch=2, \n",
    "                             weights_summary=None, \n",
    "                             progress_bar_refresh_rate=0,#verbose, \n",
    "                             min_epochs=min_epochs\n",
    "                            )\n",
    "    trainer.fit(model, \n",
    "                train_dataloaders=dm.train_dataloader(), \n",
    "                val_dataloaders=dm.val_dataloader()\n",
    "               )\n",
    "    \n",
    "    ## get val predictions\n",
    "    val_preds = model.forward( torch.tensor( X_val ).float() )[:, 1].detach().numpy()\n",
    "    \n",
    "    ## return predictions and the model\n",
    "    return( val_preds, model )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3d6ef8-56a2-4541-84d5-073b6341ae83",
   "metadata": {},
   "source": [
    "## SKLEARN Script: base only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03217122-5087-49e7-80f9-0e003b0a8318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import pairwise_distance\n",
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8db0cc0-e628-4c36-aa46-943b64d7833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_weight_feature_and_nbatchpairs_scaling(strength, df_with_batch):\n",
    "    nbs = df_with_batch.iloc[:, 0].nunique()\n",
    "    w = nbs * (nbs - 1) / 2\n",
    "    return(strength /( w * ( df_with_batch.shape[1] - 1 ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30fa664c-69e5-4b26-b353-66c3d96266c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebiasMClassifier(BaseEstimator):\n",
    "    \"\"\"DebiasMClassifier: an sklean-style wrapper for the DEBIAS-M torch implementation.\"\"\"\n",
    "    def __init__(self,\n",
    "                 *, \n",
    "                 batch_str = 'infer',\n",
    "                 learning_rate=0.005, \n",
    "                 min_epochs=25,\n",
    "                 l2_strength=0,\n",
    "                 w_l2=0,\n",
    "                 random_state=None,\n",
    "                 x_val=0\n",
    "                 ):\n",
    "        \n",
    "        self.learning_rate=learning_rate\n",
    "        self.min_epochs=min_epochs\n",
    "        self.l2_strength=l2_strength\n",
    "        self.w_l2=w_l2\n",
    "        self.batch_str=batch_str\n",
    "        self.x_val = x_val\n",
    "        self.random_state=random_state\n",
    "        \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"Fit the baseline classifier.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns the instance itself.\n",
    "        \"\"\"\n",
    "        if self.batch_str=='infer':\n",
    "            self.batch_str = batch_weight_feature_and_nbatchpairs_scaling(1e4, pd.DataFrame(X) )\n",
    "            \n",
    "        self.classes_ = np.unique(y)\n",
    "        preds, mod = DEBIASM_train_and_pred(\n",
    "                                            X, \n",
    "                                            self.x_val, \n",
    "                                            y, \n",
    "                                            0,\n",
    "                                            batch_sim_strength = self.batch_str,\n",
    "                                            learning_rate=self.learning_rate,\n",
    "                                            min_epochs= self.min_epochs,\n",
    "                                            l2_strength=self.l2_strength,\n",
    "                                            w_l2 = self.w_l2\n",
    "                                            )\n",
    "        self.model = mod\n",
    "        self.val_preds = preds\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Perform classification on test vectors X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Test data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "            Predicted target values for X.\n",
    "        \"\"\"\n",
    "        y = self.model.forward( torch.tensor( X ).float() ).detach().numpy()[:, 1]>0.5\n",
    "        return y\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Return probability estimates for the test vectors X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Test data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        P : ndarray of shape (n_samples, n_classes) or list of such arrays\n",
    "            Returns the probability of the sample for each class in\n",
    "            the model, where classes are ordered arithmetically, for each\n",
    "            output.\n",
    "        \"\"\"\n",
    "        \n",
    "        P = self.model.forward( torch.tensor( X ).float() ).detach().numpy()\n",
    "        \n",
    "        return P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e666e549-2b49-4754-acba-3e65685a730e",
   "metadata": {},
   "source": [
    "## Test script content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2300e1a-7a00-4a9a-9be4-ee7fc532f323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4, 696, 286, 226, 551],\n",
       "       [  4, 513, 666, 105, 130],\n",
       "       [  3, 542,  66, 653, 996],\n",
       "       [  3,  16, 721,   7,  84],\n",
       "       [  1, 456, 279, 932, 314]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "n_samples = 96*5\n",
    "n_batches = 5\n",
    "n_features = 100\n",
    "\n",
    "## the read count matrix\n",
    "X = ( np.random.rand(n_samples, n_features) * 1000 ).astype(int)\n",
    "\n",
    "## the labels\n",
    "y = np.random.rand(n_samples)>0.5\n",
    "\n",
    "## the batches\n",
    "batches = ( np.random.rand(n_samples) * n_batches ).astype(int)\n",
    "\n",
    "X_with_batch = np.hstack((batches[:, np.newaxis], X))\n",
    "X_with_batch[:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56c74cd4-75f8-46f6-96e2-e3efcd13bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the valdiation batch to '4'\n",
    "val_inds = batches==4\n",
    "X_train, X_val = X_with_batch[~val_inds], X_with_batch[val_inds]\n",
    "y_train, y_val = y[~val_inds], y[val_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27661eb7-aa90-43f3-9036-60629c78cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmc = DebiasMClassifier(x_val=X_val) ## give it the held-out inputs to account for\n",
    "                                    ## those domains shifts while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc7e9fe6-7fdc-41b4-b7a3-be65e3b4e1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmc.w_l2 \n",
    "\n",
    "# init to zero, single value\n",
    "# still zero after fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1771fd44-998a-4aa1-b2dd-e93ed93649df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DebiasMClassifier(x_val=array([[  4, 696, 286, ..., 398, 240, 343],\n",
       "       [  4, 513, 666, ..., 744, 472, 121],\n",
       "       [  4, 896, 223, ..., 347,   4, 294],\n",
       "       ...,\n",
       "       [  4, 648, 845, ..., 791, 976, 578],\n",
       "       [  4, 318, 960, ..., 640, 879, 992],\n",
       "       [  4, 618, 300, ..., 912, 996, 621]]))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DebiasMClassifier</label><div class=\"sk-toggleable__content\"><pre>DebiasMClassifier(x_val=array([[  4, 696, 286, ..., 398, 240, 343],\n",
       "       [  4, 513, 666, ..., 744, 472, 121],\n",
       "       [  4, 896, 223, ..., 347,   4, 294],\n",
       "       ...,\n",
       "       [  4, 648, 845, ..., 791, 976, 578],\n",
       "       [  4, 318, 960, ..., 640, 879, 992],\n",
       "       [  4, 618, 300, ..., 912, 996, 621]]))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DebiasMClassifier(x_val=array([[  4, 696, 286, ..., 398, 240, 343],\n",
       "       [  4, 513, 666, ..., 744, 472, 121],\n",
       "       [  4, 896, 223, ..., 347,   4, 294],\n",
       "       ...,\n",
       "       [  4, 648, 845, ..., 791, 976, 578],\n",
       "       [  4, 318, 960, ..., 640, 879, 992],\n",
       "       [  4, 618, 300, ..., 912, 996, 621]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c91c14a9-3684-4c48-944b-088e98b9b82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0439, -0.0139,  0.1708, -0.1313,  0.0761, -0.1367,  0.1163,  0.0613,\n",
       "          0.1345,  0.1039,  0.1212,  0.0678, -0.0489, -0.0657,  0.1246,  0.0138,\n",
       "          0.0152, -0.0360, -0.0909, -0.0733,  0.0362, -0.0414, -0.1912, -0.0985,\n",
       "         -0.0687,  0.0607,  0.0506,  0.0653,  0.1270,  0.1072, -0.0203,  0.0625,\n",
       "         -0.1134,  0.0399,  0.0888,  0.0363,  0.0021,  0.1288, -0.0716, -0.0016,\n",
       "          0.0871,  0.1119,  0.0354, -0.0106, -0.1430,  0.0335, -0.0196,  0.0220,\n",
       "          0.0846,  0.0781,  0.0415, -0.1406,  0.0696,  0.1458, -0.0699,  0.0733,\n",
       "          0.1377, -0.0186,  0.1392,  0.0299,  0.0863,  0.0697,  0.0126,  0.0205,\n",
       "          0.0285,  0.0820, -0.1087, -0.0371, -0.1158, -0.0785, -0.0591,  0.1200,\n",
       "         -0.0731,  0.1671,  0.1106,  0.0124, -0.0850,  0.0512, -0.1413,  0.0956,\n",
       "          0.0716, -0.1063, -0.0589, -0.1374, -0.0438, -0.0302, -0.0325, -0.0791,\n",
       "         -0.0830, -0.1128,  0.1007, -0.0124,  0.0999, -0.0620, -0.0127, -0.0087,\n",
       "         -0.1206,  0.0593,  0.0468, -0.0476],\n",
       "        [ 0.0195,  0.1062, -0.1774, -0.1370, -0.0493,  0.1271,  0.1528, -0.0372,\n",
       "         -0.1357, -0.0281, -0.0807,  0.1053, -0.0951,  0.0310, -0.1234, -0.0489,\n",
       "          0.0056,  0.0588, -0.0916, -0.0369, -0.0311,  0.0859,  0.0444,  0.0264,\n",
       "         -0.0518, -0.0916, -0.0755, -0.0243,  0.0145, -0.0020, -0.0133,  0.0398,\n",
       "          0.1149, -0.1112,  0.0405, -0.0780, -0.1662, -0.0902, -0.0727,  0.0878,\n",
       "         -0.0722,  0.0354, -0.0461, -0.0629, -0.0896,  0.0116, -0.0508,  0.0458,\n",
       "          0.0345, -0.0597, -0.0289,  0.1431,  0.0210,  0.1911,  0.0814, -0.1065,\n",
       "         -0.0627, -0.0377, -0.0777, -0.1172, -0.0690, -0.0674,  0.0648, -0.0946,\n",
       "         -0.0550,  0.1461, -0.0701,  0.0595,  0.0412,  0.0299, -0.1016,  0.0013,\n",
       "         -0.0612,  0.1762, -0.1191,  0.0545, -0.0479, -0.0335,  0.0015, -0.1555,\n",
       "          0.0928, -0.0211,  0.0711, -0.1330, -0.0792, -0.0210,  0.0686, -0.0393,\n",
       "          0.0368, -0.0475, -0.0774, -0.1123, -0.0967, -0.0987,  0.0549,  0.0572,\n",
       "         -0.1209,  0.0669,  0.0575, -0.0578],\n",
       "        [-0.0052, -0.0587,  0.0024, -0.0915, -0.0004, -0.0226,  0.0064,  0.0179,\n",
       "         -0.0747, -0.0765,  0.0533, -0.0959, -0.0358,  0.0713, -0.0147,  0.0495,\n",
       "          0.0051,  0.0219, -0.0641,  0.0385,  0.0800, -0.0943, -0.0075,  0.1169,\n",
       "          0.0208,  0.0995,  0.1112,  0.0872, -0.1239, -0.0239,  0.0578,  0.0544,\n",
       "          0.0242,  0.1166, -0.0105,  0.0572, -0.0966, -0.1192, -0.0090, -0.0803,\n",
       "          0.0184,  0.0745,  0.0821, -0.0012,  0.1305,  0.1151,  0.0582, -0.0690,\n",
       "          0.0347, -0.0021, -0.0920, -0.0388, -0.0265, -0.1468,  0.0042, -0.1468,\n",
       "         -0.0914, -0.0541, -0.1491,  0.1176, -0.0807, -0.0027, -0.0618, -0.1116,\n",
       "          0.0085, -0.1441,  0.1169,  0.0057, -0.0606,  0.1018, -0.0334, -0.0606,\n",
       "         -0.1058, -0.1538,  0.0242,  0.0685,  0.1157, -0.0212,  0.1406,  0.1490,\n",
       "          0.0301,  0.1192,  0.0056,  0.1105, -0.0581, -0.1093, -0.0689, -0.0258,\n",
       "         -0.0634, -0.0593,  0.0319,  0.0082,  0.0474,  0.0929, -0.0503,  0.0871,\n",
       "          0.1536, -0.1103,  0.0710,  0.0383],\n",
       "        [-0.0249,  0.0598,  0.1177,  0.0314,  0.0537,  0.0607,  0.0571,  0.0711,\n",
       "         -0.0631,  0.0161, -0.0905, -0.0345,  0.0930, -0.0989,  0.0147,  0.0545,\n",
       "         -0.0579, -0.0700, -0.1099,  0.0189,  0.0490, -0.0672, -0.0225, -0.0573,\n",
       "          0.0691,  0.0853, -0.0958,  0.0439, -0.0679, -0.0804,  0.0771,  0.0252,\n",
       "          0.0666,  0.0969,  0.1365,  0.0842,  0.1615, -0.0527,  0.1139, -0.0074,\n",
       "          0.0088, -0.0911, -0.0645,  0.0720,  0.1216, -0.0694,  0.0733, -0.0142,\n",
       "         -0.0645,  0.0829,  0.1100,  0.0609, -0.0246, -0.1884,  0.0024, -0.0547,\n",
       "          0.0004,  0.1252, -0.0143,  0.0201, -0.0355, -0.0436, -0.0065,  0.0167,\n",
       "          0.0670,  0.0337,  0.0814,  0.0250,  0.1271,  0.0137,  0.0921, -0.0986,\n",
       "          0.1000, -0.0581, -0.0695,  0.0859, -0.0376, -0.0422, -0.0058,  0.0064,\n",
       "         -0.0778, -0.0629, -0.0315, -0.0662,  0.0759,  0.0051, -0.0286, -0.0666,\n",
       "          0.1094, -0.0075, -0.0644,  0.0951, -0.0123, -0.0084,  0.0163, -0.0758,\n",
       "         -0.0112,  0.1211,  0.0679, -0.0696],\n",
       "        [ 0.0688, -0.0973,  0.1296,  0.1339, -0.0601,  0.0528, -0.1467, -0.0562,\n",
       "         -0.0919,  0.0125, -0.0140, -0.0288,  0.0454,  0.1207, -0.0332, -0.0181,\n",
       "          0.0823,  0.0840,  0.1107,  0.0970, -0.0682, -0.0564,  0.1896,  0.0107,\n",
       "          0.0843,  0.1137,  0.0853, -0.0710, -0.0457, -0.0058, -0.0566,  0.0085,\n",
       "         -0.0474,  0.0366, -0.1490,  0.0913,  0.0869,  0.0551, -0.1046, -0.0116,\n",
       "          0.0077, -0.0293, -0.0720, -0.0123, -0.0381, -0.0817,  0.0188,  0.0747,\n",
       "          0.0677, -0.0761,  0.0716, -0.0376, -0.0021,  0.0107, -0.0167,  0.1373,\n",
       "         -0.1537, -0.1159, -0.0545, -0.0702, -0.0756,  0.0742, -0.0043,  0.1174,\n",
       "         -0.0409,  0.0567,  0.0136, -0.0172,  0.0656,  0.0353, -0.0261,  0.0560,\n",
       "         -0.0760, -0.1850,  0.0891, -0.0730, -0.1167,  0.0684, -0.0166, -0.0035,\n",
       "          0.0619, -0.1112,  0.0110,  0.1478, -0.0649,  0.1129,  0.0349,  0.0953,\n",
       "         -0.0639,  0.1150,  0.0834,  0.1105, -0.0468, -0.0654,  0.0356, -0.0418,\n",
       "         -0.1617, -0.0253,  0.0722,  0.0873]], requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmc.model.batch_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e08793b5-21b3-490c-be20-4e57a454fe3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dfrybrum/anaconda3/envs/debias/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "/Users/dfrybrum/anaconda3/envs/debias/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:167: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/Users/dfrybrum/anaconda3/envs/debias/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/var/folders/zw/ktw6tcq52bg052fqwjgzj7kr0000gn/T/ipykernel_88093/3076996943.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y_hat = softmax(x)\n",
      "/Users/dfrybrum/anaconda3/envs/debias/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/dfrybrum/anaconda3/envs/debias/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:432: UserWarning: The number of training samples (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Trainer was signaled to stop but required minimum epochs (25) or minimum steps (None) has not been met. Training will continue...\n",
      "Trainer was signaled to stop but required minimum epochs (25) or minimum steps (None) has not been met. Training will continue...\n",
      "Trainer was signaled to stop but required minimum epochs (25) or minimum steps (None) has not been met. Training will continue...\n",
      "Trainer was signaled to stop but required minimum epochs (25) or minimum steps (None) has not been met. Training will continue...\n",
      "Trainer was signaled to stop but required minimum epochs (25) or minimum steps (None) has not been met. Training will continue...\n",
      "Trainer was signaled to stop but required minimum epochs (25) or minimum steps (None) has not been met. Training will continue...\n",
      "Trainer was signaled to stop but required minimum epochs (25) or minimum steps (None) has not been met. Training will continue...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DebiasMClassifier(batch_str=16.666666666666668,\n",
       "                  x_val=array([[  4, 696, 286, ..., 398, 240, 343],\n",
       "       [  4, 513, 666, ..., 744, 472, 121],\n",
       "       [  4, 896, 223, ..., 347,   4, 294],\n",
       "       ...,\n",
       "       [  4, 648, 845, ..., 791, 976, 578],\n",
       "       [  4, 318, 960, ..., 640, 879, 992],\n",
       "       [  4, 618, 300, ..., 912, 996, 621]]))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DebiasMClassifier</label><div class=\"sk-toggleable__content\"><pre>DebiasMClassifier(batch_str=16.666666666666668,\n",
       "                  x_val=array([[  4, 696, 286, ..., 398, 240, 343],\n",
       "       [  4, 513, 666, ..., 744, 472, 121],\n",
       "       [  4, 896, 223, ..., 347,   4, 294],\n",
       "       ...,\n",
       "       [  4, 648, 845, ..., 791, 976, 578],\n",
       "       [  4, 318, 960, ..., 640, 879, 992],\n",
       "       [  4, 618, 300, ..., 912, 996, 621]]))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DebiasMClassifier(batch_str=16.666666666666668,\n",
       "                  x_val=array([[  4, 696, 286, ..., 398, 240, 343],\n",
       "       [  4, 513, 666, ..., 744, 472, 121],\n",
       "       [  4, 896, 223, ..., 347,   4, 294],\n",
       "       ...,\n",
       "       [  4, 648, 845, ..., 791, 976, 578],\n",
       "       [  4, 318, 960, ..., 640, 879, 992],\n",
       "       [  4, 618, 300, ..., 912, 996, 621]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmc.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
